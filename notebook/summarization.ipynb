{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install transformers\n!pip install datasets\n!pip install sentencepiece\n!pip install rouge_score\n!pip install simplet5 -q","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-11-02T00:58:18.793819Z","iopub.execute_input":"2023-11-02T00:58:18.794170Z","iopub.status.idle":"2023-11-02T00:59:41.328897Z","shell.execute_reply.started":"2023-11-02T00:58:18.794141Z","shell.execute_reply":"2023-11-02T00:59:41.327578Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.33.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.12.2)\nRequirement already satisfied: huggingface-hub<1.0,>=0.15.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.16.4)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.23.5)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.6.3)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.31.0)\nRequirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.13.3)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.3.3)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.1)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (2023.9.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (4.6.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.0.9)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.1.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2023.7.22)\nRequirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (2.1.0)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from datasets) (1.23.5)\nRequirement already satisfied: pyarrow>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (11.0.0)\nRequirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.7)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.0.2)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (2.31.0)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (4.66.1)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.3.0)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.15)\nRequirement already satisfied: fsspec[http]>=2021.05.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (2023.9.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.8.4)\nRequirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.16.4)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets) (21.3)\nRequirement already satisfied: responses<0.19 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.18.0)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.1.0)\nRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (3.1.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.2)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.2)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.3)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.12.2)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (6.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (4.6.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->datasets) (3.0.9)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (2023.7.22)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.3)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.3)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\nRequirement already satisfied: sentencepiece in /opt/conda/lib/python3.10/site-packages (0.1.99)\nCollecting rouge_score\n  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: absl-py in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.4.0)\nRequirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (from rouge_score) (3.2.4)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.23.5)\nRequirement already satisfied: six>=1.14.0 in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.16.0)\nBuilding wheels for collected packages: rouge_score\n  Building wheel for rouge_score (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=60802403acbd578f58c55ee0eae6b86dce53bcd6d73398268ac826a2af7979ae\n  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\nSuccessfully built rouge_score\nInstalling collected packages: rouge_score\nSuccessfully installed rouge_score-0.1.2\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nconda 23.7.3 requires setuptools>=60.0.0, but you have setuptools 59.5.0 which is incompatible.\nopentelemetry-api 1.18.0 requires importlib-metadata~=6.0.0, but you have importlib-metadata 6.7.0 which is incompatible.\npymc3 3.11.5 requires numpy<1.22.2,>=1.15.0, but you have numpy 1.23.5 which is incompatible.\npymc3 3.11.5 requires scipy<1.8.0,>=1.7.3, but you have scipy 1.11.2 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, Seq2SeqTrainingArguments, Seq2SeqTrainer, DataCollatorForSeq2Seq\nfrom transformers import T5Tokenizer, T5ForConditionalGeneration, T5Config\nfrom simplet5 import SimpleT5\nfrom tabulate import tabulate\nfrom datasets import Dataset\nimport nltk\nfrom datetime import datetime\nimport numpy as np\nimport pickle\nimport datasets","metadata":{"execution":{"iopub.status.busy":"2023-11-02T00:59:41.331006Z","iopub.execute_input":"2023-11-02T00:59:41.331287Z","iopub.status.idle":"2023-11-02T00:59:57.868203Z","shell.execute_reply.started":"2023-11-02T00:59:41.331262Z","shell.execute_reply":"2023-11-02T00:59:57.867120Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}]},{"cell_type":"code","source":"\nimport os\nos.environ[\"WANDB_DISABLED\"] = \"true\"","metadata":{"execution":{"iopub.status.busy":"2023-11-02T00:59:57.870090Z","iopub.execute_input":"2023-11-02T00:59:57.870474Z","iopub.status.idle":"2023-11-02T00:59:57.875341Z","shell.execute_reply.started":"2023-11-02T00:59:57.870439Z","shell.execute_reply":"2023-11-02T00:59:57.874395Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"Train = open(\"/kaggle/input/rankar-data/Ranked-data/Train-cross-encoder.pickle\",\"rb\")\nTrain_data = pickle.load(Train)","metadata":{"execution":{"iopub.status.busy":"2023-11-02T00:59:57.876678Z","iopub.execute_input":"2023-11-02T00:59:57.877307Z","iopub.status.idle":"2023-11-02T00:59:57.992023Z","shell.execute_reply.started":"2023-11-02T00:59:57.877274Z","shell.execute_reply":"2023-11-02T00:59:57.990888Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Val = open(\"/kaggle/input/rankar-data/Ranked-data/Val-cross-encoder.pickle\",\"rb\")\nVal_data = pickle.load(Val)","metadata":{"execution":{"iopub.status.busy":"2023-11-02T00:59:57.995715Z","iopub.execute_input":"2023-11-02T00:59:57.996343Z","iopub.status.idle":"2023-11-02T00:59:58.020627Z","shell.execute_reply.started":"2023-11-02T00:59:57.996311Z","shell.execute_reply":"2023-11-02T00:59:58.019749Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"Test = open(\"/kaggle/input/rankar-data/Ranked-data/Test-cross-encoder.pickle\",\"rb\")\nTest_data = pickle.load(Test)","metadata":{"execution":{"iopub.status.busy":"2023-11-02T00:59:58.021731Z","iopub.execute_input":"2023-11-02T00:59:58.022012Z","iopub.status.idle":"2023-11-02T00:59:58.051215Z","shell.execute_reply.started":"2023-11-02T00:59:58.021987Z","shell.execute_reply":"2023-11-02T00:59:58.050439Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport datasets\n\n# Assuming Train_data, Val_data, and Test_data are lists of dictionaries\ntrain_df = pd.DataFrame(Train_data)\nval_df = pd.DataFrame(Val_data)\ntest_df = pd.DataFrame(Test_data)\n","metadata":{"execution":{"iopub.status.busy":"2023-11-02T00:59:58.052436Z","iopub.execute_input":"2023-11-02T00:59:58.053110Z","iopub.status.idle":"2023-11-02T00:59:58.065161Z","shell.execute_reply.started":"2023-11-02T00:59:58.053074Z","shell.execute_reply":"2023-11-02T00:59:58.064365Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# Convert Pandas DataFrames to datasets\ntrain_data_txt = datasets.Dataset.from_pandas(train_df)\nval_data_txt = datasets.Dataset.from_pandas(val_df)\ntest_data_txt = datasets.Dataset.from_pandas(test_df)","metadata":{"execution":{"iopub.status.busy":"2023-11-02T00:59:58.066335Z","iopub.execute_input":"2023-11-02T00:59:58.066705Z","iopub.status.idle":"2023-11-02T00:59:58.125937Z","shell.execute_reply.started":"2023-11-02T00:59:58.066674Z","shell.execute_reply":"2023-11-02T00:59:58.125155Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"def flatten(example):\n  answers = example[\"rankedanswers\"]\n#  print(answers[0])\n  allrankedanswers = \"\"\n  for answer in answers:\n    allrankedanswers += answer + \" \"\n    \n  return {\n      \n      \"question\": example[\"question\"],\n      \"allrankedanswers\": allrankedanswers,\n      \"firstsummary\": example[\"firstsummary\"],\n      \"question+allrankedanswers\": example[\"question\"] + \" \" + allrankedanswers\n      }","metadata":{"execution":{"iopub.status.busy":"2023-11-02T00:59:58.126995Z","iopub.execute_input":"2023-11-02T00:59:58.127278Z","iopub.status.idle":"2023-11-02T00:59:58.132850Z","shell.execute_reply.started":"2023-11-02T00:59:58.127254Z","shell.execute_reply":"2023-11-02T00:59:58.131841Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"train_data_txt = train_data_txt.map(flatten, remove_columns=['rankedanswers'])\nval_data_txt = val_data_txt.map(flatten, remove_columns=['rankedanswers'])\ntest_data_txt = test_data_txt.map(flatten, remove_columns=['rankedanswers'])","metadata":{"execution":{"iopub.status.busy":"2023-11-02T00:59:58.134122Z","iopub.execute_input":"2023-11-02T00:59:58.134439Z","iopub.status.idle":"2023-11-02T00:59:58.842133Z","shell.execute_reply.started":"2023-11-02T00:59:58.134402Z","shell.execute_reply":"2023-11-02T00:59:58.841243Z"},"trusted":true},"execution_count":10,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2783 [00:00<?, ?ex/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"59d9629d750c4022ab0f576a34aece9a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/500 [00:00<?, ?ex/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3e62d331f3734b95b7aabe271e612d62"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1000 [00:00<?, ?ex/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6ec836d4b9a941ae959763884df84a60"}},"metadata":{}}]},{"cell_type":"code","source":"len(val_data_txt)","metadata":{"execution":{"iopub.status.busy":"2023-11-02T00:59:58.843533Z","iopub.execute_input":"2023-11-02T00:59:58.843976Z","iopub.status.idle":"2023-11-02T00:59:58.851350Z","shell.execute_reply.started":"2023-11-02T00:59:58.843937Z","shell.execute_reply":"2023-11-02T00:59:58.850559Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"500"},"metadata":{}}]},{"cell_type":"code","source":"\nval_data_txt[0]\n","metadata":{"execution":{"iopub.status.busy":"2023-11-02T00:59:58.852382Z","iopub.execute_input":"2023-11-02T00:59:58.852682Z","iopub.status.idle":"2023-11-02T00:59:58.863009Z","shell.execute_reply.started":"2023-11-02T00:59:58.852659Z","shell.execute_reply":"2023-11-02T00:59:58.862104Z"},"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"{'question': 'Assume that someone (friend, family) asks you to format his/her CV. They typically might want to apply to a job where text processing etc. is not one of the key requirements and they ask you to do it in order to have a nice result and/or to win time, although they could probably do it themselves in Word.  Can it be negative for them? Imagine for instance that the interviewer asks them how they made such a nice CV.',\n 'firstsummary': \"Saving seeds for next year is not difficult as long as you aren't using hybrid varieties. Some plants are hard to get seeds from if you live in a cold climate or require special techniques.\",\n 'allrankedanswers': 'Is it acceptable to format a cv for someone else? Nobody knows or cares who wrote or formatted your CV/resume. Having someone else format it for you is likely to lead them to a false conclusion. If the C.V. or job description describes the person as being good at design, formatting documents, or experienced with LaTeX, then the person should format it themselves. I help friends write and format their resumes all the time. It is absolutely acceptable to proofread someone\\'s CV/resume. Yes , its fine to help someone with their resume/cv. Formatting and content both matter . I have been complimented on my resume for its format and such in the past, but never has an interviewer asked how I came up with it. To avoid this, make sure you go over the CV as you are making the changes, and the person understands them and can go over it with you line by line and memorize it. In this (very limited) case, the implicit assumption the company will come away with is that you are presenting your C.V. as an example of your work. But if they did, saying \"Well, I had a friend who is really good at it You want your resume to be fine tuned to hit the requirements of the job for which you are applying. It doesn\\'t \"fit\" the job applied for. It\\'s fairly normal to ask someone to proof I can\\'t imagine that an interviewer would ask about a nice resume. They only care that it accurately reflects you, your background, and your career. If he doesn\\'t know what\\'s on his own resume, that\\'s a BIG red flag -read your resume or to help with layout, so there\\'s nothing wrong there. Yes, of course. ',\n 'question+allrankedanswers': 'Assume that someone (friend, family) asks you to format his/her CV. They typically might want to apply to a job where text processing etc. is not one of the key requirements and they ask you to do it in order to have a nice result and/or to win time, although they could probably do it themselves in Word.  Can it be negative for them? Imagine for instance that the interviewer asks them how they made such a nice CV. Is it acceptable to format a cv for someone else? Nobody knows or cares who wrote or formatted your CV/resume. Having someone else format it for you is likely to lead them to a false conclusion. If the C.V. or job description describes the person as being good at design, formatting documents, or experienced with LaTeX, then the person should format it themselves. I help friends write and format their resumes all the time. It is absolutely acceptable to proofread someone\\'s CV/resume. Yes , its fine to help someone with their resume/cv. Formatting and content both matter . I have been complimented on my resume for its format and such in the past, but never has an interviewer asked how I came up with it. To avoid this, make sure you go over the CV as you are making the changes, and the person understands them and can go over it with you line by line and memorize it. In this (very limited) case, the implicit assumption the company will come away with is that you are presenting your C.V. as an example of your work. But if they did, saying \"Well, I had a friend who is really good at it You want your resume to be fine tuned to hit the requirements of the job for which you are applying. It doesn\\'t \"fit\" the job applied for. It\\'s fairly normal to ask someone to proof I can\\'t imagine that an interviewer would ask about a nice resume. They only care that it accurately reflects you, your background, and your career. If he doesn\\'t know what\\'s on his own resume, that\\'s a BIG red flag -read your resume or to help with layout, so there\\'s nothing wrong there. Yes, of course. '}"},"metadata":{}}]},{"cell_type":"code","source":"# Model-Bart trained on X-Sum\n\nmodel_name = \"sshleifer/distilbart-xsum-12-3\"\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_name)\ntokenizer = AutoTokenizer.from_pretrained(model_name)","metadata":{"execution":{"iopub.status.busy":"2023-11-02T00:59:58.864260Z","iopub.execute_input":"2023-11-02T00:59:58.864836Z","iopub.status.idle":"2023-11-02T01:00:15.916590Z","shell.execute_reply.started":"2023-11-02T00:59:58.864804Z","shell.execute_reply":"2023-11-02T01:00:15.915696Z"},"trusted":true},"execution_count":13,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/1.48k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f716366b86844a87a6be8288ce0ca068"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/683M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c17cb6150c344fcb9b1e2434d1abe921"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/26.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fbd8f824831b402ca212a9cfdc3b09ff"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/878k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fecaaef7699f42209b2a093157112ae3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/446k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9f94109bf0384b75b827f5cfb51e3955"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/772 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1fe3fb20b0354501b603a8c2c146e87f"}},"metadata":{}}]},{"cell_type":"code","source":"# Model T-5\n#model = T5ForConditionalGeneration.from_pretrained('t5-small')\n#tokenizer = T5Tokenizer.from_pretrained('t5-base')\n","metadata":{"execution":{"iopub.status.busy":"2023-11-02T01:00:42.724237Z","iopub.execute_input":"2023-11-02T01:00:42.725163Z","iopub.status.idle":"2023-11-02T01:00:42.729146Z","shell.execute_reply.started":"2023-11-02T01:00:42.725125Z","shell.execute_reply":"2023-11-02T01:00:42.728095Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"encoder_max_length = 512\ndecoder_max_length = 100\ninput = \"allrankedanswers\"\noutput = \"firstsummary\"\n\ntrain_data_txt.column_names\n\ntraining_args = Seq2SeqTrainingArguments(\n    output_dir=\"results\",\n    num_train_epochs=5,  # demo\n    do_train=True,\n    do_eval=True,\n    per_device_train_batch_size=2,  # demo\n    per_device_eval_batch_size=2,\n    # learning_rate=3e-05,\n    warmup_steps=100,\n    weight_decay=0.1,\n    label_smoothing_factor=0.1,\n    predict_with_generate=True,\n    logging_dir=\"logs\",\n    logging_steps=200,    ## it take lot of output space  so i incresead logging step 50 to 200\n    save_total_limit=1,\n)","metadata":{"execution":{"iopub.status.busy":"2023-11-02T01:06:50.187912Z","iopub.execute_input":"2023-11-02T01:06:50.188561Z","iopub.status.idle":"2023-11-02T01:06:50.199461Z","shell.execute_reply.started":"2023-11-02T01:06:50.188527Z","shell.execute_reply":"2023-11-02T01:06:50.198366Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stderr","text":"PyTorch: setting up devices\nThe default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\nUsing the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n","output_type":"stream"}]},{"cell_type":"code","source":"def batch_tokenize_preprocess(batch, tokenizer, max_source_length, max_target_length):\n    source, target = batch[input], batch[output]\n    source_tokenized = tokenizer(\n        source, padding=\"max_length\", truncation=True, max_length=max_source_length\n    )\n    target_tokenized = tokenizer(\n        target, padding=\"max_length\", truncation=True, max_length=max_target_length\n    )\n\n    batch = {k: v for k, v in source_tokenized.items()}\n    # Ignore padding in the loss\n    batch[\"labels\"] = [\n        [-100 if token == tokenizer.pad_token_id else token for token in l]\n        for l in target_tokenized[\"input_ids\"]\n    ]\n    return batch","metadata":{"execution":{"iopub.status.busy":"2023-11-02T01:06:55.937224Z","iopub.execute_input":"2023-11-02T01:06:55.938074Z","iopub.status.idle":"2023-11-02T01:06:55.945446Z","shell.execute_reply.started":"2023-11-02T01:06:55.938032Z","shell.execute_reply":"2023-11-02T01:06:55.944454Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"train_data = train_data_txt.map(\n    lambda batch: batch_tokenize_preprocess(\n        batch, tokenizer, encoder_max_length, decoder_max_length\n    ),\n    batched=True,\n    remove_columns=train_data_txt.column_names,\n)","metadata":{"execution":{"iopub.status.busy":"2023-11-02T01:06:56.900858Z","iopub.execute_input":"2023-11-02T01:06:56.901569Z","iopub.status.idle":"2023-11-02T01:06:59.902954Z","shell.execute_reply.started":"2023-11-02T01:06:56.901535Z","shell.execute_reply":"2023-11-02T01:06:59.902111Z"},"trusted":true},"execution_count":29,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b95e984a8abb4acc8180f23b34755fc4"}},"metadata":{}}]},{"cell_type":"code","source":"val_data = val_data_txt.map(\n    lambda batch: batch_tokenize_preprocess(\n        batch, tokenizer, encoder_max_length, decoder_max_length\n    ),\n    batched=True,\n    remove_columns=val_data_txt.column_names,\n)","metadata":{"execution":{"iopub.status.busy":"2023-11-02T01:06:59.904543Z","iopub.execute_input":"2023-11-02T01:06:59.904862Z","iopub.status.idle":"2023-11-02T01:07:00.459726Z","shell.execute_reply.started":"2023-11-02T01:06:59.904824Z","shell.execute_reply":"2023-11-02T01:07:00.458792Z"},"trusted":true},"execution_count":30,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"10910985ebcf42e0afe16517307bc874"}},"metadata":{}}]},{"cell_type":"code","source":"test_data = test_data_txt.map(\n    lambda batch: batch_tokenize_preprocess(\n        batch, tokenizer, encoder_max_length, decoder_max_length\n    ),\n    batched=True,\n    remove_columns=test_data_txt.column_names,\n)","metadata":{"execution":{"iopub.status.busy":"2023-11-02T01:07:00.460858Z","iopub.execute_input":"2023-11-02T01:07:00.461155Z","iopub.status.idle":"2023-11-02T01:07:01.533325Z","shell.execute_reply.started":"2023-11-02T01:07:00.461128Z","shell.execute_reply":"2023-11-02T01:07:01.531983Z"},"trusted":true},"execution_count":31,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2b38da8cdbda45368e884d15b78f5ec4"}},"metadata":{}}]},{"cell_type":"code","source":"#METRICS\nnltk.download(\"punkt\", quiet=True)\n\nmetric = datasets.load_metric(\"rouge\")\n\ndef postprocess_text(preds, labels):\n    preds = [pred.strip() for pred in preds]\n    labels = [label.strip() for label in labels]\n\n    # rougeLSum expects newline after each sentence\n    preds = [\"\\n\".join(nltk.sent_tokenize(pred)) for pred in preds]\n    labels = [\"\\n\".join(nltk.sent_tokenize(label)) for label in labels]\n\n    return preds, labels","metadata":{"execution":{"iopub.status.busy":"2023-11-02T01:07:01.535615Z","iopub.execute_input":"2023-11-02T01:07:01.535948Z","iopub.status.idle":"2023-11-02T01:07:01.782494Z","shell.execute_reply.started":"2023-11-02T01:07:01.535919Z","shell.execute_reply":"2023-11-02T01:07:01.781410Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"def compute_metrics(eval_preds):\n    preds, labels = eval_preds\n    if isinstance(preds, tuple):\n        preds = preds[0]\n    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n    # Replace -100 in the labels as we can't decode them.\n    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n\n    # Some simple post-processing\n    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n\n    result = metric.compute(\n        predictions=decoded_preds, references=decoded_labels, use_stemmer=True\n    )\n    # Extract a few results from ROUGE\n    result = {key: value.mid.fmeasure * 100 for key, value in result.items()}\n\n    prediction_lens = [\n        np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds\n    ]\n    result[\"gen_len\"] = np.mean(prediction_lens)\n    result = {k: round(v, 4) for k, v in result.items()}\n    return result\n","metadata":{"execution":{"iopub.status.busy":"2023-11-02T01:07:01.783834Z","iopub.execute_input":"2023-11-02T01:07:01.784223Z","iopub.status.idle":"2023-11-02T01:07:01.793097Z","shell.execute_reply.started":"2023-11-02T01:07:01.784188Z","shell.execute_reply":"2023-11-02T01:07:01.792039Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)","metadata":{"execution":{"iopub.status.busy":"2023-11-02T01:07:03.308184Z","iopub.execute_input":"2023-11-02T01:07:03.308986Z","iopub.status.idle":"2023-11-02T01:07:03.313585Z","shell.execute_reply.started":"2023-11-02T01:07:03.308949Z","shell.execute_reply":"2023-11-02T01:07:03.312490Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"trainer = Seq2SeqTrainer(\n    model=model,\n    args=training_args,\n    data_collator=data_collator,\n    train_dataset=train_data,\n    eval_dataset=val_data,\n    tokenizer=tokenizer,\n    compute_metrics=compute_metrics,\n)","metadata":{"execution":{"iopub.status.busy":"2023-11-02T01:07:04.282284Z","iopub.execute_input":"2023-11-02T01:07:04.282683Z","iopub.status.idle":"2023-11-02T01:07:04.302312Z","shell.execute_reply.started":"2023-11-02T01:07:04.282651Z","shell.execute_reply":"2023-11-02T01:07:04.301321Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"\n#EVALUATE BEFORE FINE TUNING\ntrainer.evaluate()","metadata":{"execution":{"iopub.status.busy":"2023-11-02T01:07:05.224701Z","iopub.execute_input":"2023-11-02T01:07:05.225083Z","iopub.status.idle":"2023-11-02T01:08:37.251498Z","shell.execute_reply.started":"2023-11-02T01:07:05.225052Z","shell.execute_reply":"2023-11-02T01:08:37.250473Z"},"trusted":true},"execution_count":36,"outputs":[{"name":"stderr","text":"***** Running Evaluation *****\n  Num examples = 500\n  Batch size = 4\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='250' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [125/125 45:49]\n    </div>\n    "},"metadata":{}},{"execution_count":36,"output_type":"execute_result","data":{"text/plain":"{'eval_loss': 6.488889694213867,\n 'eval_rouge1': 12.6251,\n 'eval_rouge2': 1.3119,\n 'eval_rougeL': 10.3298,\n 'eval_rougeLsum': 11.0816,\n 'eval_gen_len': 14.904,\n 'eval_runtime': 92.0145,\n 'eval_samples_per_second': 5.434,\n 'eval_steps_per_second': 1.358}"},"metadata":{}}]},{"cell_type":"code","source":"#TRAIN THE MODEL\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2023-11-02T01:08:37.253656Z","iopub.execute_input":"2023-11-02T01:08:37.254005Z","iopub.status.idle":"2023-11-02T01:50:16.077934Z","shell.execute_reply.started":"2023-11-02T01:08:37.253970Z","shell.execute_reply":"2023-11-02T01:50:16.077016Z"},"trusted":true},"execution_count":37,"outputs":[{"name":"stderr","text":"***** Running training *****\n  Num examples = 2783\n  Num Epochs = 5\n  Instantaneous batch size per device = 2\n  Total train batch size (w. parallel, distributed & accumulation) = 4\n  Gradient Accumulation steps = 1\n  Total optimization steps = 3480\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='3480' max='3480' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [3480/3480 41:38, Epoch 5/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>200</td>\n      <td>3.191000</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>1.906100</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>1.745900</td>\n    </tr>\n    <tr>\n      <td>800</td>\n      <td>1.677700</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>1.654400</td>\n    </tr>\n    <tr>\n      <td>1200</td>\n      <td>1.654500</td>\n    </tr>\n    <tr>\n      <td>1400</td>\n      <td>1.616900</td>\n    </tr>\n    <tr>\n      <td>1600</td>\n      <td>1.605100</td>\n    </tr>\n    <tr>\n      <td>1800</td>\n      <td>1.609500</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>1.596800</td>\n    </tr>\n    <tr>\n      <td>2200</td>\n      <td>1.592600</td>\n    </tr>\n    <tr>\n      <td>2400</td>\n      <td>1.579800</td>\n    </tr>\n    <tr>\n      <td>2600</td>\n      <td>1.582300</td>\n    </tr>\n    <tr>\n      <td>2800</td>\n      <td>1.578300</td>\n    </tr>\n    <tr>\n      <td>3000</td>\n      <td>1.557400</td>\n    </tr>\n    <tr>\n      <td>3200</td>\n      <td>1.556900</td>\n    </tr>\n    <tr>\n      <td>3400</td>\n      <td>1.555300</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"Saving model checkpoint to results/checkpoint-500\nConfiguration saved in results/checkpoint-500/config.json\nModel weights saved in results/checkpoint-500/pytorch_model.bin\ntokenizer config file saved in results/checkpoint-500/tokenizer_config.json\nSpecial tokens file saved in results/checkpoint-500/special_tokens_map.json\nSaving model checkpoint to results/checkpoint-1000\nConfiguration saved in results/checkpoint-1000/config.json\nModel weights saved in results/checkpoint-1000/pytorch_model.bin\ntokenizer config file saved in results/checkpoint-1000/tokenizer_config.json\nSpecial tokens file saved in results/checkpoint-1000/special_tokens_map.json\nDeleting older checkpoint [results/checkpoint-500] due to args.save_total_limit\nSaving model checkpoint to results/checkpoint-1500\nConfiguration saved in results/checkpoint-1500/config.json\nModel weights saved in results/checkpoint-1500/pytorch_model.bin\ntokenizer config file saved in results/checkpoint-1500/tokenizer_config.json\nSpecial tokens file saved in results/checkpoint-1500/special_tokens_map.json\nDeleting older checkpoint [results/checkpoint-1000] due to args.save_total_limit\nSaving model checkpoint to results/checkpoint-2000\nConfiguration saved in results/checkpoint-2000/config.json\nModel weights saved in results/checkpoint-2000/pytorch_model.bin\ntokenizer config file saved in results/checkpoint-2000/tokenizer_config.json\nSpecial tokens file saved in results/checkpoint-2000/special_tokens_map.json\nDeleting older checkpoint [results/checkpoint-1500] due to args.save_total_limit\nSaving model checkpoint to results/checkpoint-2500\nConfiguration saved in results/checkpoint-2500/config.json\nModel weights saved in results/checkpoint-2500/pytorch_model.bin\ntokenizer config file saved in results/checkpoint-2500/tokenizer_config.json\nSpecial tokens file saved in results/checkpoint-2500/special_tokens_map.json\nDeleting older checkpoint [results/checkpoint-2000] due to args.save_total_limit\nSaving model checkpoint to results/checkpoint-3000\nConfiguration saved in results/checkpoint-3000/config.json\nModel weights saved in results/checkpoint-3000/pytorch_model.bin\ntokenizer config file saved in results/checkpoint-3000/tokenizer_config.json\nSpecial tokens file saved in results/checkpoint-3000/special_tokens_map.json\nDeleting older checkpoint [results/checkpoint-2500] due to args.save_total_limit\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\n","output_type":"stream"},{"execution_count":37,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=3480, training_loss=1.7170055805951698, metrics={'train_runtime': 2498.799, 'train_samples_per_second': 5.569, 'train_steps_per_second': 1.393, 'total_flos': 8615582367744000.0, 'train_loss': 1.7170055805951698, 'epoch': 5.0})"},"metadata":{}}]},{"cell_type":"code","source":"\n#EVALUATE AFTER FINE TUNING\ntrainer.evaluate()","metadata":{"execution":{"iopub.status.busy":"2023-11-02T01:50:50.867562Z","iopub.execute_input":"2023-11-02T01:50:50.867946Z","iopub.status.idle":"2023-11-02T01:52:57.951658Z","shell.execute_reply.started":"2023-11-02T01:50:50.867916Z","shell.execute_reply":"2023-11-02T01:52:57.950755Z"},"trusted":true},"execution_count":38,"outputs":[{"name":"stderr","text":"***** Running Evaluation *****\n  Num examples = 500\n  Batch size = 4\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"execution_count":38,"output_type":"execute_result","data":{"text/plain":"{'eval_loss': 7.032856464385986,\n 'eval_rouge1': 14.1805,\n 'eval_rouge2': 1.041,\n 'eval_rougeL': 10.2796,\n 'eval_rougeLsum': 12.7389,\n 'eval_gen_len': 38.228,\n 'eval_runtime': 127.0731,\n 'eval_samples_per_second': 3.935,\n 'eval_steps_per_second': 0.984,\n 'epoch': 5.0}"},"metadata":{}}]},{"cell_type":"code","source":"#Generate summaries from the fine-tuned model and compare them with those generated from the original, pre-trained one.\ndef generate_summary(test_samples, model):\n    inputs = tokenizer(\n        test_samples[\"allrankedanswers\"],\n        padding=\"max_length\",\n        truncation=True,\n        max_length=encoder_max_length,\n        return_tensors=\"pt\",\n    )\n    input_ids = inputs.input_ids.to(model.device)\n    attention_mask = inputs.attention_mask.to(model.device)\n    outputs = model.generate(input_ids, attention_mask=attention_mask)\n    output_str = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n    return outputs, output_str","metadata":{"execution":{"iopub.status.busy":"2023-11-02T01:54:04.994458Z","iopub.execute_input":"2023-11-02T01:54:04.995255Z","iopub.status.idle":"2023-11-02T01:54:05.001594Z","shell.execute_reply.started":"2023-11-02T01:54:04.995221Z","shell.execute_reply":"2023-11-02T01:54:05.000452Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"model_before_tuning = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n#model_before_tuning = T5ForConditionalGeneration.from_pretrained('t5-small')\ntest_samples = val_data_txt.select(range(5))\nmodel_after_training = AutoModelForSeq2SeqLM.from_pretrained(\"sshleifer/distilbart-xsum-12-3\")\nsummaries_before_tuning = generate_summary(test_samples, model_before_tuning)[1]\nsummaries_after_tuning = generate_summary(test_samples, model)[1]\nsummaries_after_training = generate_summary(test_samples, model_after_training)[1]","metadata":{"execution":{"iopub.status.busy":"2023-11-02T01:56:02.940532Z","iopub.execute_input":"2023-11-02T01:56:02.941797Z","iopub.status.idle":"2023-11-02T01:56:31.578737Z","shell.execute_reply.started":"2023-11-02T01:56:02.941747Z","shell.execute_reply":"2023-11-02T01:56:31.577781Z"},"trusted":true},"execution_count":41,"outputs":[{"name":"stderr","text":"loading configuration file https://huggingface.co/sshleifer/distilbart-xsum-12-3/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/4a0f7fb35f1504b6e865136124e3781fb488792aa105a84a991a3145a027791f.10ebe969457e130b9da526e7994b6191d3765d1d01ac6abc2eb20bb8adcbd4e0\nModel config BartConfig {\n  \"_name_or_path\": \"sshleifer/distilbart-xsum-12-3\",\n  \"_num_labels\": 3,\n  \"activation_dropout\": 0.0,\n  \"activation_function\": \"gelu\",\n  \"add_bias_logits\": false,\n  \"add_final_layer_norm\": false,\n  \"architectures\": [\n    \"BartForConditionalGeneration\"\n  ],\n  \"attention_dropout\": 0.1,\n  \"bos_token_id\": 0,\n  \"classif_dropout\": 0.0,\n  \"classifier_dropout\": 0.0,\n  \"d_model\": 1024,\n  \"decoder_attention_heads\": 16,\n  \"decoder_ffn_dim\": 4096,\n  \"decoder_layerdrop\": 0.0,\n  \"decoder_layers\": 3,\n  \"decoder_start_token_id\": 2,\n  \"dropout\": 0.1,\n  \"early_stopping\": true,\n  \"encoder_attention_heads\": 16,\n  \"encoder_ffn_dim\": 4096,\n  \"encoder_layerdrop\": 0.0,\n  \"encoder_layers\": 12,\n  \"eos_token_id\": 2,\n  \"eos_token_ids\": [\n    2\n  ],\n  \"extra_pos_embeddings\": 2,\n  \"force_bos_token_to_be_generated\": false,\n  \"forced_eos_token_id\": 2,\n  \"gradient_checkpointing\": false,\n  \"id2label\": {\n    \"0\": \"LABEL_0\",\n    \"1\": \"LABEL_1\",\n    \"2\": \"LABEL_2\"\n  },\n  \"init_std\": 0.02,\n  \"is_encoder_decoder\": true,\n  \"label2id\": {\n    \"LABEL_0\": 0,\n    \"LABEL_1\": 1,\n    \"LABEL_2\": 2\n  },\n  \"max_length\": 62,\n  \"max_position_embeddings\": 1024,\n  \"min_length\": 11,\n  \"model_type\": \"bart\",\n  \"no_repeat_ngram_size\": 3,\n  \"normalize_before\": false,\n  \"normalize_embedding\": true,\n  \"num_beams\": 6,\n  \"num_hidden_layers\": 12,\n  \"output_past\": true,\n  \"pad_token_id\": 1,\n  \"prefix\": \" \",\n  \"save_step\": 58,\n  \"scale_embedding\": false,\n  \"static_position_embeddings\": false,\n  \"task_specific_params\": {},\n  \"transformers_version\": \"4.16.2\",\n  \"use_cache\": true,\n  \"vocab_size\": 50264\n}\n\nloading weights file https://huggingface.co/sshleifer/distilbart-xsum-12-3/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/7a77a8f9ea9d26fd3fe82afa42497e4011beea00978a78665f42f20cdb7cbc2e.f73389b704edc5b1e1dfc2888bebeac96caaeb8b7d9827d4d03e726594b11f6a\nAll model checkpoint weights were used when initializing BartForConditionalGeneration.\n\nAll the weights of BartForConditionalGeneration were initialized from the model checkpoint at sshleifer/distilbart-xsum-12-3.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use BartForConditionalGeneration for predictions without further training.\nloading configuration file https://huggingface.co/sshleifer/distilbart-xsum-12-3/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/4a0f7fb35f1504b6e865136124e3781fb488792aa105a84a991a3145a027791f.10ebe969457e130b9da526e7994b6191d3765d1d01ac6abc2eb20bb8adcbd4e0\nModel config BartConfig {\n  \"_name_or_path\": \"sshleifer/distilbart-xsum-12-3\",\n  \"_num_labels\": 3,\n  \"activation_dropout\": 0.0,\n  \"activation_function\": \"gelu\",\n  \"add_bias_logits\": false,\n  \"add_final_layer_norm\": false,\n  \"architectures\": [\n    \"BartForConditionalGeneration\"\n  ],\n  \"attention_dropout\": 0.1,\n  \"bos_token_id\": 0,\n  \"classif_dropout\": 0.0,\n  \"classifier_dropout\": 0.0,\n  \"d_model\": 1024,\n  \"decoder_attention_heads\": 16,\n  \"decoder_ffn_dim\": 4096,\n  \"decoder_layerdrop\": 0.0,\n  \"decoder_layers\": 3,\n  \"decoder_start_token_id\": 2,\n  \"dropout\": 0.1,\n  \"early_stopping\": true,\n  \"encoder_attention_heads\": 16,\n  \"encoder_ffn_dim\": 4096,\n  \"encoder_layerdrop\": 0.0,\n  \"encoder_layers\": 12,\n  \"eos_token_id\": 2,\n  \"eos_token_ids\": [\n    2\n  ],\n  \"extra_pos_embeddings\": 2,\n  \"force_bos_token_to_be_generated\": false,\n  \"forced_eos_token_id\": 2,\n  \"gradient_checkpointing\": false,\n  \"id2label\": {\n    \"0\": \"LABEL_0\",\n    \"1\": \"LABEL_1\",\n    \"2\": \"LABEL_2\"\n  },\n  \"init_std\": 0.02,\n  \"is_encoder_decoder\": true,\n  \"label2id\": {\n    \"LABEL_0\": 0,\n    \"LABEL_1\": 1,\n    \"LABEL_2\": 2\n  },\n  \"max_length\": 62,\n  \"max_position_embeddings\": 1024,\n  \"min_length\": 11,\n  \"model_type\": \"bart\",\n  \"no_repeat_ngram_size\": 3,\n  \"normalize_before\": false,\n  \"normalize_embedding\": true,\n  \"num_beams\": 6,\n  \"num_hidden_layers\": 12,\n  \"output_past\": true,\n  \"pad_token_id\": 1,\n  \"prefix\": \" \",\n  \"save_step\": 58,\n  \"scale_embedding\": false,\n  \"static_position_embeddings\": false,\n  \"task_specific_params\": {},\n  \"transformers_version\": \"4.16.2\",\n  \"use_cache\": true,\n  \"vocab_size\": 50264\n}\n\nloading weights file https://huggingface.co/sshleifer/distilbart-xsum-12-3/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/7a77a8f9ea9d26fd3fe82afa42497e4011beea00978a78665f42f20cdb7cbc2e.f73389b704edc5b1e1dfc2888bebeac96caaeb8b7d9827d4d03e726594b11f6a\nAll model checkpoint weights were used when initializing BartForConditionalGeneration.\n\nAll the weights of BartForConditionalGeneration were initialized from the model checkpoint at sshleifer/distilbart-xsum-12-3.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use BartForConditionalGeneration for predictions without further training.\n","output_type":"stream"}]},{"cell_type":"code","source":"for i in range(5):\n  print(val_data_txt['firstsummary'][i])\n\n# print(summaries_before_tuning)\n\n# print(summaries_after_tuning)","metadata":{"execution":{"iopub.status.busy":"2023-11-02T01:57:53.385478Z","iopub.execute_input":"2023-11-02T01:57:53.385860Z","iopub.status.idle":"2023-11-02T01:57:53.396119Z","shell.execute_reply.started":"2023-11-02T01:57:53.385832Z","shell.execute_reply":"2023-11-02T01:57:53.395137Z"},"trusted":true},"execution_count":45,"outputs":[{"name":"stdout","text":"Saving seeds for next year is not difficult as long as you aren't using hybrid varieties. Some plants are hard to get seeds from if you live in a cold climate or require special techniques.\nThey won't have much of an effect on your plants however they do feed aphids.\nThe cartridges don't fit smoothly into the slot and the box is not official. They also might not work on newer versions of the console. The cartridge could also be bigger or a different shade of grey.\nNobody will check this valuation unless you are audited, so you just need to be able to justify the amount. You can look for similar items for sale on eBay. The IRS isn't going to quibble if your numbers are reasonable.\nThey are quite useful. Some high-level plays build extra Fortresses in Yellow Mine Expansions.\n","output_type":"stream"}]},{"cell_type":"code","source":"print(summaries_before_tuning)","metadata":{"execution":{"iopub.status.busy":"2023-11-02T01:57:33.565617Z","iopub.execute_input":"2023-11-02T01:57:33.566050Z","iopub.status.idle":"2023-11-02T01:57:33.571478Z","shell.execute_reply.started":"2023-11-02T01:57:33.566019Z","shell.execute_reply":"2023-11-02T01:57:33.570247Z"},"trusted":true},"execution_count":43,"outputs":[{"name":"stdout","text":"[\" On the eve of a job interview, I've been asking someone to proofread your resume.\", ' In our series of letters from African-American journalists, film-maker and columnist Farai Sevenzo considers whether to send a resume by mail or courier.', \" An employee's notice period is not ethical, and it's not ethical.\", ' There is a kind of \"trick\" question to see if one jumps into giving a technical answer to the question.', ' In our series of letters from African journalists, film-maker and columnist Farai Sevenzo looks at the challenges faced by people living in your office.']\n","output_type":"stream"}]},{"cell_type":"code","source":"print(summaries_after_tuning)","metadata":{"execution":{"iopub.status.busy":"2023-11-02T01:57:45.099135Z","iopub.execute_input":"2023-11-02T01:57:45.099543Z","iopub.status.idle":"2023-11-02T01:57:45.104838Z","shell.execute_reply.started":"2023-11-02T01:57:45.099512Z","shell.execute_reply":"2023-11-02T01:57:45.103688Z"},"trusted":true},"execution_count":44,"outputs":[{"name":"stdout","text":"['It is not necessary to bring up this information if you are not asked about it but if they ask you about it then you should definitely answer honestly. You could bring it up in the interview to save the company being surprised by it.', \"You should not assume anything and make sure you check your contract. However, if you haven't been let go already then there is a good chance you have passed your probation.\", 'You should inform the customer that they should call your work number and leave a message. You could even block them from calling you on that number. Discuss the issue with your management.', \"The best way is to immediately correct them so they don't make the same mistake again. You could also introduce yourself with the right title at the start of the call. If it is someone you probably won't speak to again, then just ignore it.\", \"Don't worry about it. Think about your interview technique for future applications. Don't consider offers from companies that have already done this to you.\"]\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}